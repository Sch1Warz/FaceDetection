{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import skimage.io\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "from keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications.nasnet import NASNetLarge\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                   validation_split = 0.2,\n",
    "                                  \n",
    "        rotation_range=5,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        #zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
    "                                  validation_split = 0.2)\n",
    "\n",
    "test_datagen  = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22968 images belonging to 7 classes.\n",
      "Found 5741 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"../archive/train\" #passing the path with training images\n",
    "test_dir = \"../archive/test\"   #passing the path with testing images\n",
    "train_dataset  = train_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                   target_size = (48,48),\n",
    "                                                   class_mode = 'categorical',\n",
    "                                                   subset = 'training',\n",
    "                                                   batch_size = 64)\n",
    "valid_dataset = valid_datagen.flow_from_directory(directory = train_dir,\n",
    "                                                  target_size = (48,48),\n",
    "                                                  class_mode = 'categorical',\n",
    "                                                  subset = 'validation',\n",
    "                                                  batch_size = 64)\n",
    "test_dataset = test_datagen.flow_from_directory(directory = test_dir,\n",
    "                                                  target_size = (48,48),\n",
    "                                                  class_mode = 'categorical',\n",
    "                                                  batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.VGG16(input_shape=(48,48,3),include_top=False,weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 1, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                16416     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32)               128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 231       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,735,879\n",
      "Trainable params: 7,099,399\n",
      "Non-trainable params: 7,636,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building Model\n",
    "\n",
    "model=tf.keras.models.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32,kernel_initializer='he_uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "359/359 [==============================] - 20s 52ms/step - loss: 1.2550 - accuracy: 0.5358 - val_loss: 1.3784 - val_accuracy: 0.4963\n",
      "Epoch 2/100\n",
      "359/359 [==============================] - 18s 49ms/step - loss: 1.2524 - accuracy: 0.5361 - val_loss: 1.3830 - val_accuracy: 0.4992\n",
      "Epoch 3/100\n",
      "359/359 [==============================] - 18s 51ms/step - loss: 1.2594 - accuracy: 0.5368 - val_loss: 1.3351 - val_accuracy: 0.5118\n",
      "Epoch 4/100\n",
      "359/359 [==============================] - 18s 51ms/step - loss: 1.2511 - accuracy: 0.5365 - val_loss: 1.3735 - val_accuracy: 0.5086\n",
      "Epoch 5/100\n",
      "359/359 [==============================] - 18s 50ms/step - loss: 1.2481 - accuracy: 0.5383 - val_loss: 1.3333 - val_accuracy: 0.5149\n",
      "Epoch 6/100\n",
      "359/359 [==============================] - 20s 54ms/step - loss: 1.2494 - accuracy: 0.5399 - val_loss: 1.3359 - val_accuracy: 0.5158\n",
      "Epoch 7/100\n",
      "359/359 [==============================] - 31s 88ms/step - loss: 1.2493 - accuracy: 0.5431 - val_loss: 1.3790 - val_accuracy: 0.4970\n",
      "Epoch 8/100\n",
      "359/359 [==============================] - 43s 119ms/step - loss: 1.2460 - accuracy: 0.5423 - val_loss: 1.3326 - val_accuracy: 0.5159\n",
      "Epoch 9/100\n",
      "359/359 [==============================] - 44s 122ms/step - loss: 1.2365 - accuracy: 0.5477 - val_loss: 1.3353 - val_accuracy: 0.5072\n",
      "Epoch 10/100\n",
      "359/359 [==============================] - 41s 115ms/step - loss: 1.2515 - accuracy: 0.5396 - val_loss: 1.3446 - val_accuracy: 0.5179\n",
      "Epoch 11/100\n",
      "359/359 [==============================] - 41s 114ms/step - loss: 1.3044 - accuracy: 0.5176 - val_loss: 1.3429 - val_accuracy: 0.5137\n",
      "Epoch 12/100\n",
      "359/359 [==============================] - 39s 109ms/step - loss: 1.2481 - accuracy: 0.5395 - val_loss: 1.3365 - val_accuracy: 0.5191\n",
      "Epoch 13/100\n",
      "359/359 [==============================] - 40s 110ms/step - loss: 1.2304 - accuracy: 0.5484 - val_loss: 1.3413 - val_accuracy: 0.5135\n",
      "Epoch 14/100\n",
      "359/359 [==============================] - 40s 110ms/step - loss: 1.2327 - accuracy: 0.5490 - val_loss: 1.3538 - val_accuracy: 0.5135\n",
      "Epoch 15/100\n",
      "359/359 [==============================] - 40s 110ms/step - loss: 1.2359 - accuracy: 0.5448 - val_loss: 1.3371 - val_accuracy: 0.5145\n",
      "Epoch 16/100\n",
      "359/359 [==============================] - 40s 110ms/step - loss: 1.2318 - accuracy: 0.5483 - val_loss: 1.3364 - val_accuracy: 0.5152\n",
      "Epoch 17/100\n",
      "359/359 [==============================] - 40s 111ms/step - loss: 1.2367 - accuracy: 0.5440 - val_loss: 1.3195 - val_accuracy: 0.5189\n",
      "Epoch 18/100\n",
      "359/359 [==============================] - 40s 112ms/step - loss: 1.2235 - accuracy: 0.5539 - val_loss: 1.3190 - val_accuracy: 0.5243\n",
      "Epoch 19/100\n",
      "359/359 [==============================] - 40s 111ms/step - loss: 1.2220 - accuracy: 0.5491 - val_loss: 1.3461 - val_accuracy: 0.5145\n",
      "Epoch 20/100\n",
      "359/359 [==============================] - 40s 110ms/step - loss: 1.2218 - accuracy: 0.5536 - val_loss: 1.3258 - val_accuracy: 0.5212\n",
      "Epoch 21/100\n",
      "359/359 [==============================] - 39s 110ms/step - loss: 1.2273 - accuracy: 0.5499 - val_loss: 1.3149 - val_accuracy: 0.5224\n",
      "Epoch 22/100\n",
      "359/359 [==============================] - 39s 110ms/step - loss: 1.2271 - accuracy: 0.5520 - val_loss: 1.3296 - val_accuracy: 0.5166\n",
      "Epoch 23/100\n",
      "359/359 [==============================] - 39s 108ms/step - loss: 1.2172 - accuracy: 0.5574 - val_loss: 1.3522 - val_accuracy: 0.5165\n",
      "Epoch 24/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.2216 - accuracy: 0.5539 - val_loss: 1.3694 - val_accuracy: 0.5050\n",
      "Epoch 25/100\n",
      "359/359 [==============================] - 39s 109ms/step - loss: 1.2147 - accuracy: 0.5568 - val_loss: 1.3338 - val_accuracy: 0.5152\n",
      "Epoch 26/100\n",
      "359/359 [==============================] - 39s 107ms/step - loss: 1.2068 - accuracy: 0.5599 - val_loss: 1.3608 - val_accuracy: 0.5067\n",
      "Epoch 27/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.2069 - accuracy: 0.5606 - val_loss: 1.3241 - val_accuracy: 0.5165\n",
      "Epoch 28/100\n",
      "359/359 [==============================] - 39s 108ms/step - loss: 1.2098 - accuracy: 0.5570 - val_loss: 1.3404 - val_accuracy: 0.5114\n",
      "Epoch 29/100\n",
      "359/359 [==============================] - 40s 110ms/step - loss: 1.2119 - accuracy: 0.5592 - val_loss: 1.3317 - val_accuracy: 0.5132\n",
      "Epoch 30/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.2037 - accuracy: 0.5628 - val_loss: 1.3223 - val_accuracy: 0.5274\n",
      "Epoch 31/100\n",
      "359/359 [==============================] - 39s 108ms/step - loss: 1.2059 - accuracy: 0.5635 - val_loss: 1.3416 - val_accuracy: 0.5104\n",
      "Epoch 32/100\n",
      "359/359 [==============================] - 39s 108ms/step - loss: 1.2066 - accuracy: 0.5592 - val_loss: 1.3362 - val_accuracy: 0.5198\n",
      "Epoch 33/100\n",
      "359/359 [==============================] - 38s 107ms/step - loss: 1.2044 - accuracy: 0.5629 - val_loss: 1.3416 - val_accuracy: 0.5165\n",
      "Epoch 34/100\n",
      "359/359 [==============================] - 39s 109ms/step - loss: 1.2038 - accuracy: 0.5621 - val_loss: 1.3542 - val_accuracy: 0.5107\n",
      "Epoch 35/100\n",
      "359/359 [==============================] - 39s 107ms/step - loss: 1.2120 - accuracy: 0.5607 - val_loss: 1.3958 - val_accuracy: 0.4985\n",
      "Epoch 36/100\n",
      "359/359 [==============================] - 39s 109ms/step - loss: 1.2142 - accuracy: 0.5596 - val_loss: 1.3346 - val_accuracy: 0.5147\n",
      "Epoch 37/100\n",
      "359/359 [==============================] - 38s 107ms/step - loss: 1.1910 - accuracy: 0.5661 - val_loss: 1.3587 - val_accuracy: 0.4992\n",
      "Epoch 38/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.1970 - accuracy: 0.5639 - val_loss: 1.3643 - val_accuracy: 0.5130\n",
      "Epoch 39/100\n",
      "359/359 [==============================] - 38s 107ms/step - loss: 1.1845 - accuracy: 0.5697 - val_loss: 1.3127 - val_accuracy: 0.5266\n",
      "Epoch 40/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.1895 - accuracy: 0.5678 - val_loss: 1.3207 - val_accuracy: 0.5238\n",
      "Epoch 41/100\n",
      "359/359 [==============================] - 39s 109ms/step - loss: 1.1851 - accuracy: 0.5692 - val_loss: 1.3533 - val_accuracy: 0.5050\n",
      "Epoch 42/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.1892 - accuracy: 0.5655 - val_loss: 1.3185 - val_accuracy: 0.5229\n",
      "Epoch 43/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.1837 - accuracy: 0.5699 - val_loss: 1.3313 - val_accuracy: 0.5227\n",
      "Epoch 44/100\n",
      "359/359 [==============================] - 38s 107ms/step - loss: 1.1799 - accuracy: 0.5729 - val_loss: 1.3366 - val_accuracy: 0.5191\n",
      "Epoch 45/100\n",
      "359/359 [==============================] - 39s 108ms/step - loss: 1.2050 - accuracy: 0.5627 - val_loss: 1.3168 - val_accuracy: 0.5229\n",
      "Epoch 46/100\n",
      "359/359 [==============================] - 39s 109ms/step - loss: 1.1778 - accuracy: 0.5753 - val_loss: 1.3141 - val_accuracy: 0.5231\n",
      "Epoch 47/100\n",
      "359/359 [==============================] - 39s 107ms/step - loss: 1.1801 - accuracy: 0.5747 - val_loss: 1.3212 - val_accuracy: 0.5206\n",
      "Epoch 48/100\n",
      "359/359 [==============================] - 38s 106ms/step - loss: 1.1726 - accuracy: 0.5786 - val_loss: 1.3439 - val_accuracy: 0.5208\n",
      "Epoch 49/100\n",
      "359/359 [==============================] - 34s 96ms/step - loss: 1.1813 - accuracy: 0.5719 - val_loss: 1.3243 - val_accuracy: 0.5252\n",
      "Epoch 50/100\n",
      "359/359 [==============================] - 29s 80ms/step - loss: 1.1728 - accuracy: 0.5754 - val_loss: 1.3099 - val_accuracy: 0.5243\n",
      "Epoch 51/100\n",
      "359/359 [==============================] - 23s 64ms/step - loss: 1.1770 - accuracy: 0.5737 - val_loss: 1.3235 - val_accuracy: 0.5222\n",
      "Epoch 52/100\n",
      "359/359 [==============================] - 22s 61ms/step - loss: 1.1608 - accuracy: 0.5815 - val_loss: 1.3511 - val_accuracy: 0.5093\n",
      "Epoch 53/100\n",
      "359/359 [==============================] - 21s 59ms/step - loss: 1.1673 - accuracy: 0.5784 - val_loss: 1.3373 - val_accuracy: 0.5165\n",
      "Epoch 54/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1593 - accuracy: 0.5836 - val_loss: 1.3294 - val_accuracy: 0.5206\n",
      "Epoch 55/100\n",
      "359/359 [==============================] - 24s 66ms/step - loss: 1.1627 - accuracy: 0.5808 - val_loss: 1.3457 - val_accuracy: 0.5180\n",
      "Epoch 56/100\n",
      "359/359 [==============================] - 24s 68ms/step - loss: 1.1620 - accuracy: 0.5853 - val_loss: 1.3259 - val_accuracy: 0.5294\n",
      "Epoch 57/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1589 - accuracy: 0.5802 - val_loss: 1.3217 - val_accuracy: 0.5257\n",
      "Epoch 58/100\n",
      "359/359 [==============================] - 26s 73ms/step - loss: 1.1608 - accuracy: 0.5821 - val_loss: 1.3256 - val_accuracy: 0.5222\n",
      "Epoch 59/100\n",
      "359/359 [==============================] - 28s 79ms/step - loss: 1.1576 - accuracy: 0.5829 - val_loss: 1.3187 - val_accuracy: 0.5245\n",
      "Epoch 60/100\n",
      "359/359 [==============================] - 31s 85ms/step - loss: 1.1583 - accuracy: 0.5834 - val_loss: 1.2951 - val_accuracy: 0.5306\n",
      "Epoch 61/100\n",
      "359/359 [==============================] - 26s 73ms/step - loss: 1.1503 - accuracy: 0.5876 - val_loss: 1.3160 - val_accuracy: 0.5217\n",
      "Epoch 62/100\n",
      "359/359 [==============================] - 26s 74ms/step - loss: 1.1518 - accuracy: 0.5873 - val_loss: 1.3287 - val_accuracy: 0.5236\n",
      "Epoch 63/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1844 - accuracy: 0.5718 - val_loss: 1.3133 - val_accuracy: 0.5328\n",
      "Epoch 64/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1481 - accuracy: 0.5867 - val_loss: 1.3123 - val_accuracy: 0.5304\n",
      "Epoch 65/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1588 - accuracy: 0.5825 - val_loss: 1.3325 - val_accuracy: 0.5233\n",
      "Epoch 66/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1467 - accuracy: 0.5887 - val_loss: 1.3381 - val_accuracy: 0.5252\n",
      "Epoch 67/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1413 - accuracy: 0.5913 - val_loss: 1.3576 - val_accuracy: 0.5192\n",
      "Epoch 68/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1475 - accuracy: 0.5898 - val_loss: 1.3775 - val_accuracy: 0.5058\n",
      "Epoch 69/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1389 - accuracy: 0.5898 - val_loss: 1.3478 - val_accuracy: 0.5180\n",
      "Epoch 70/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1467 - accuracy: 0.5903 - val_loss: 1.3089 - val_accuracy: 0.5309\n",
      "Epoch 71/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1455 - accuracy: 0.5910 - val_loss: 1.3218 - val_accuracy: 0.5269\n",
      "Epoch 72/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1233 - accuracy: 0.5987 - val_loss: 1.3502 - val_accuracy: 0.5212\n",
      "Epoch 73/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1505 - accuracy: 0.5888 - val_loss: 1.3413 - val_accuracy: 0.5168\n",
      "Epoch 74/100\n",
      "359/359 [==============================] - 27s 76ms/step - loss: 1.1348 - accuracy: 0.5949 - val_loss: 1.3183 - val_accuracy: 0.5278\n",
      "Epoch 75/100\n",
      "359/359 [==============================] - 27s 76ms/step - loss: 1.1353 - accuracy: 0.5930 - val_loss: 1.3216 - val_accuracy: 0.5316\n",
      "Epoch 76/100\n",
      "359/359 [==============================] - 27s 76ms/step - loss: 1.1298 - accuracy: 0.5964 - val_loss: 1.3272 - val_accuracy: 0.5243\n",
      "Epoch 77/100\n",
      "359/359 [==============================] - 27s 76ms/step - loss: 1.1299 - accuracy: 0.5970 - val_loss: 1.3230 - val_accuracy: 0.5281\n",
      "Epoch 78/100\n",
      "359/359 [==============================] - 27s 76ms/step - loss: 1.1247 - accuracy: 0.5978 - val_loss: 1.3026 - val_accuracy: 0.5287\n",
      "Epoch 79/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1271 - accuracy: 0.5999 - val_loss: 1.3170 - val_accuracy: 0.5367\n",
      "Epoch 80/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1279 - accuracy: 0.5991 - val_loss: 1.3089 - val_accuracy: 0.5363\n",
      "Epoch 81/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1253 - accuracy: 0.5948 - val_loss: 1.3155 - val_accuracy: 0.5313\n",
      "Epoch 82/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1234 - accuracy: 0.5994 - val_loss: 1.3079 - val_accuracy: 0.5321\n",
      "Epoch 83/100\n",
      "359/359 [==============================] - 27s 76ms/step - loss: 1.1252 - accuracy: 0.6007 - val_loss: 1.3195 - val_accuracy: 0.5300\n",
      "Epoch 84/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1284 - accuracy: 0.5997 - val_loss: 1.3276 - val_accuracy: 0.5287\n",
      "Epoch 85/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1195 - accuracy: 0.6018 - val_loss: 1.3201 - val_accuracy: 0.5267\n",
      "Epoch 86/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1268 - accuracy: 0.5981 - val_loss: 1.2985 - val_accuracy: 0.5311\n",
      "Epoch 87/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1162 - accuracy: 0.6008 - val_loss: 1.3263 - val_accuracy: 0.5260\n",
      "Epoch 88/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1110 - accuracy: 0.6038 - val_loss: 1.2944 - val_accuracy: 0.5363\n",
      "Epoch 89/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1095 - accuracy: 0.6102 - val_loss: 1.3538 - val_accuracy: 0.5255\n",
      "Epoch 90/100\n",
      "359/359 [==============================] - 27s 74ms/step - loss: 1.1122 - accuracy: 0.6023 - val_loss: 1.3112 - val_accuracy: 0.5325\n",
      "Epoch 91/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1205 - accuracy: 0.6028 - val_loss: 1.3048 - val_accuracy: 0.5330\n",
      "Epoch 92/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1062 - accuracy: 0.6127 - val_loss: 1.3155 - val_accuracy: 0.5323\n",
      "Epoch 93/100\n",
      "359/359 [==============================] - 28s 78ms/step - loss: 1.1106 - accuracy: 0.6080 - val_loss: 1.3096 - val_accuracy: 0.5353\n",
      "Epoch 94/100\n",
      "359/359 [==============================] - 26s 73ms/step - loss: 1.1123 - accuracy: 0.6081 - val_loss: 1.3208 - val_accuracy: 0.5320\n",
      "Epoch 95/100\n",
      "359/359 [==============================] - 26s 73ms/step - loss: 1.0974 - accuracy: 0.6087 - val_loss: 1.3183 - val_accuracy: 0.5318\n",
      "Epoch 96/100\n",
      "359/359 [==============================] - 27s 75ms/step - loss: 1.1026 - accuracy: 0.6118 - val_loss: 1.3211 - val_accuracy: 0.5288\n",
      "Epoch 97/100\n",
      "359/359 [==============================] - 23s 65ms/step - loss: 1.1001 - accuracy: 0.6094 - val_loss: 1.3094 - val_accuracy: 0.5320\n",
      "Epoch 98/100\n",
      "359/359 [==============================] - 22s 60ms/step - loss: 1.0999 - accuracy: 0.6128 - val_loss: 1.3118 - val_accuracy: 0.5264\n",
      "Epoch 99/100\n",
      "359/359 [==============================] - 21s 59ms/step - loss: 1.0953 - accuracy: 0.6102 - val_loss: 1.3107 - val_accuracy: 0.5325\n",
      "Epoch 100/100\n",
      "359/359 [==============================] - 21s 58ms/step - loss: 1.0995 - accuracy: 0.6114 - val_loss: 1.3293 - val_accuracy: 0.5287\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "history=model.fit(train_dataset,validation_data=valid_dataset,epochs = epochs,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('weights/weight2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2032057bfa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('weights/weight2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'auc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\28573\\Desktop\\machine\\Project\\vgg.ipynb 单元格 10\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     ax5\u001b[39m.\u001b[39mlegend([\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m Train_Val_Plot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m],history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m                history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m],history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m                history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mauc\u001b[39;49m\u001b[39m'\u001b[39;49m],history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_auc\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m                history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m],history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_precision\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m'\u001b[39m],history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_f1_score\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/28573/Desktop/machine/Project/vgg.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m               )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'auc'"
     ]
    }
   ],
   "source": [
    "def Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n",
    "    \n",
    "    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n",
    "    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n",
    "\n",
    "    ax1.plot(range(1, len(acc) + 1), acc)\n",
    "    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n",
    "    ax1.set_title('History of Accuracy')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend(['training', 'validation'])\n",
    "\n",
    "\n",
    "    ax2.plot(range(1, len(loss) + 1), loss)\n",
    "    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n",
    "    ax2.set_title('History of Loss')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend(['training', 'validation'])\n",
    "\n",
    "    ax3.plot(range(1, len(auc) + 1), auc)\n",
    "    ax3.plot(range(1, len(val_auc) + 1), val_auc)\n",
    "    ax3.set_title('History of AUC')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('AUC')\n",
    "    ax3.legend(['training', 'validation'])\n",
    "    \n",
    "    ax4.plot(range(1, len(precision) + 1), precision)\n",
    "    ax4.plot(range(1, len(val_precision) + 1), val_precision)\n",
    "    ax4.set_title('History of Precision')\n",
    "    ax4.set_xlabel('Epochs')\n",
    "    ax4.set_ylabel('Precision')\n",
    "    ax4.legend(['training', 'validation'])\n",
    "    \n",
    "    ax5.plot(range(1, len(f1) + 1), f1)\n",
    "    ax5.plot(range(1, len(val_f1) + 1), val_f1)\n",
    "    ax5.set_title('History of F1-score')\n",
    "    ax5.set_xlabel('Epochs')\n",
    "    ax5.set_ylabel('F1 score')\n",
    "    ax5.legend(['training', 'validation'])\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "Train_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n",
    "               history.history['loss'],history.history['val_loss'],\n",
    "               history.history['auc'],history.history['val_auc'],\n",
    "               history.history['precision'],history.history['val_precision'],\n",
    "               history.history['f1_score'],history.history['val_f1_score']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359/359 [==============================] - 19s 51ms/step - loss: 1.1180 - accuracy: 0.6280\n",
      "train loss: 1.1180362701416016 train accuracy: 0.6280041933059692\n",
      "90/90 [==============================] - 2s 19ms/step - loss: 1.3293 - accuracy: 0.5287\n",
      "validation loss: 1.3293477296829224 validation accuracy: 0.5286535620689392\n",
      "113/113 [==============================] - 2s 21ms/step - loss: 1.3098 - accuracy: 0.5376\n",
      "test loss: 1.3098264932632446 test accuracy: 0.5376149415969849\n"
     ]
    }
   ],
   "source": [
    "# old\n",
    "loss1, acc1 = model.evaluate(train_dataset)\n",
    "print(\"train loss:\", loss1, \"train accuracy:\", acc1)\n",
    "loss2, acc2 = model.evaluate(valid_dataset)\n",
    "print(\"validation loss:\", loss2, \"validation accuracy:\", acc2)\n",
    "loss3, acc3 = model.evaluate(test_dataset)\n",
    "print(\"test loss:\", loss3, \"test accuracy:\", acc3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
